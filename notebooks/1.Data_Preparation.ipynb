{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import boto3\n",
    "import awswrangler as wr\n",
    "import concurrent.futures\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas version\n",
    "import boto3\n",
    "print(pd.__version__)\n",
    "\n",
    "# Boto3 version\n",
    "print(boto3.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the CSV files\n",
    "# dataset_dir = '../dataset_large/amazon_co-ecommerce_sample.csv'\n",
    "\n",
    "# Si quieren practicar:\n",
    "dataset_dir = '../dataset/sample_raw_productos_por_categoria_10_total_1343_productos.csv'\n",
    "\n",
    "# Read the file, assuming the first line is a header\n",
    "df = pd.read_csv(dataset_dir, header=0, sep=',', quotechar='\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns:\n",
    "df = df.drop([\"customers_who_bought_this_item_also_bought\", \n",
    "              \"items_customers_buy_after_viewing_this_item\",\n",
    "              \"customer_questions_and_answers\",\n",
    "              \"number_of_answered_questions\",\n",
    "              \"sellers\",\n",
    "              \"description\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by amazon_category_and_sub_category\n",
    "distinct_counts = df.groupby('amazon_category_and_sub_category')['amazon_category_and_sub_category'].nunique().count()\n",
    "\n",
    "# Display the result\n",
    "print(distinct_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the category and subcategories\n",
    "category_and_subcategories = df['amazon_category_and_sub_category'].str.split(' > ', expand=True)\n",
    "\n",
    "# Determine the number of subcategories\n",
    "num_subcategories = category_and_subcategories.shape[1]\n",
    "\n",
    "# Create the new columns\n",
    "col_name = f'subcategory_1'\n",
    "for i in range(num_subcategories):\n",
    "    df[col_name] = category_and_subcategories[i]\n",
    "    col_name = f'subcategory_{i+1}'\n",
    "\n",
    "# Assign the first column as the 'category'\n",
    "df['category'] = category_and_subcategories[0]\n",
    "\n",
    "# Drop columns:\n",
    "df = df.drop([\"amazon_category_and_sub_category\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If category and subcategory_1 are null, fill it with \"others\"\n",
    "df[\"category\"] = df[\"category\"].fillna(\"others\")\n",
    "df[\"subcategory_1\"] = df[\"subcategory_1\"].fillna(\"others\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'sub_category' and count distinct values in 'name'\n",
    "distinct_counts = df.groupby('subcategory_1')['subcategory_1'].nunique().count()\n",
    "\n",
    "# Display the result\n",
    "print(distinct_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('subcategory_1')['subcategory_1'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave only the Review rate, removing the \"average_review_rating\"\n",
    "df['average_review_rating'] = df['average_review_rating'].str.replace(' out of 5 stars', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove GBP symbol from price\n",
    "df['price'] = df['price'].str.replace('£', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove \" new\" from column \"number_available_in_stock\"\n",
    "df['number_available_in_stock'] = df['number_available_in_stock'].str.extract(r'(\\d+)', expand=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's a lot of garbage in some product_information values; e.g. \"...Customer Reviews amznJQ.onReady(...\"\n",
    "df['product_information'] = df['product_information'].str.replace(r'amznJQ.*', '', regex=True)\n",
    "\n",
    "# Same for column \"description\", after a string> #productDescription\n",
    "df['product_description'] = df['product_description'].str.replace(r'#productDescription*', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As reviews can be quite long, we just get some of these:\n",
    "df['customer_reviews'] = df['customer_reviews'].str.slice(0, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename column uniq_id for product_id\n",
    "df = df.rename(columns={'uniq_id': 'product_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count some value\n",
    "df['subcategory_1'].value_counts()['Pencils']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin: Creamos subsets para pruebas, enfocado a usuarios con poca RAM, CPU, etc\n",
    "\n",
    "> No podemos hacer un df.head(), ya que los datos están ordenados. Por ej, los primeros 1000 rows son Trenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_n_per_category(df, category_column, n):\n",
    "    # Create a list to store the sampled data\n",
    "    sampled_data = []\n",
    "\n",
    "    # Iterate through each unique category\n",
    "    for category in df[category_column].unique():\n",
    "        # Get the subset of data for this category\n",
    "        category_data = df[df[category_column] == category]\n",
    "\n",
    "        # Sample n rows or all if less than n\n",
    "        sampled = category_data.sample(min(len(category_data), n))\n",
    "\n",
    "        # Append to our list\n",
    "        sampled_data.append(sampled)\n",
    "\n",
    "    # Concatenate all the sampled data\n",
    "    result = pd.concat(sampled_data, ignore_index=True)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of rows you want per subcategory\n",
    "N = 100\n",
    "\n",
    "# Create the new subset\n",
    "subset_df = sample_n_per_category(df, 'subcategory_1', N)\n",
    "\n",
    "# Write to local\n",
    "total_rows = len(subset_df)\n",
    "output_path = f\"../dataset/subset_productos_por_categoria_{N}_total_{total_rows}_productos.csv\"\n",
    "subset_df.to_csv(output_path, index=False)\n",
    "\n",
    "# Check the distribution of subcategories\n",
    "subcategory_counts = subset_df['subcategory_1'].value_counts()\n",
    "print(\"\\nSubcategory distribution:\")\n",
    "print(subcategory_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End: Creamos subsets para pruebas, enfocado a usuarios con poca RAM, CPU, etc\n",
    "\n",
    "---\n",
    "## Writing data\n",
    "\n",
    "> Creamos un nuevo DF, acorde al número de productos que queremos cargar en el Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the CSV files\n",
    "dataset_dir = '../dataset/subset_productos_por_categoria_100_total_4476_productos.csv'\n",
    "\n",
    "# Read the CSV lines, assuming the first line is a header\n",
    "df = pd.read_csv(dataset_dir, header=0, sep=',', quotechar='\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "> Opcional: Creamos una lista con los distintos subcategorías, para añadirlos a nuestra streamlit app:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get distinct values from the 'subcategory_1' column, sort them, and convert to a list\n",
    "distinct_values = sorted(df['subcategory_1'].unique().tolist())\n",
    "\n",
    "# Function to chunk the list into groups of 5\n",
    "def chunk_list(lst, chunk_size):\n",
    "    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]\n",
    "\n",
    "# Chunk the distinct values into groups of 5\n",
    "chunked_values = chunk_list(distinct_values, 5)\n",
    "\n",
    "# Write to a local file\n",
    "with open('subcategory_1_values.txt', 'w') as file:\n",
    "    file.write('[\\n')\n",
    "    for i, chunk in enumerate(chunked_values):\n",
    "        formatted_chunk = \", \".join(f'\"{value}\"' for value in chunk)\n",
    "        if i < len(chunked_values) - 1:\n",
    "            file.write('    ' + formatted_chunk + ',\\n')\n",
    "        else:\n",
    "            file.write('    ' + formatted_chunk + '\\n')\n",
    "    file.write(']')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Escribimos datos a S3, a partir del nuevo DF cargado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 settings\n",
    "s3_bucket = \"genai-carlos-contreras-bucket-data-quarks-labs-oregon-01\"\n",
    "s3_key = \"datasets/demo_kb/knowledge-base-ecommerce-s3-001/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set SDK\n",
    "s3_client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importante:\n",
    "\n",
    "- Este método usa paralelismo. El número de threads se puede cambiar para datasets mayores.\n",
    "\n",
    "- Aquí hacemos el chunking manualmente, almacenando un archivo por subcategory_1 (columna creada por nosotros)\n",
    "\n",
    "- Análisis hecho: 10 rows por archivo parece llegar al límite de Chunk Size para Titan Embeddings V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv_to_s3(args):\n",
    "    category, sub_category, s3_bucket, s3_key, num_rows_per_file = args\n",
    "    \n",
    "    # Format names\n",
    "    file_category = re.sub(r\"[,\\s&']\", \"_\", category)\n",
    "    file_subcategory = re.sub(r\"[,\\s&']\", \"_\", sub_category)\n",
    "\n",
    "    try:\n",
    "            \n",
    "        # Calculate the number of files needed\n",
    "        subset = df[(df['category'] == category) & (df['subcategory_1'] == sub_category)]\n",
    "        num_rows = len(subset)\n",
    "        num_files = ceil(num_rows / num_rows_per_file)\n",
    "\n",
    "        # Create the files\n",
    "        for i in range(num_files):\n",
    "            start_row = i * num_rows_per_file\n",
    "            end_row = min((i + 1) * num_rows_per_file, num_rows)\n",
    "            file_name = f\"{file_category}_{file_subcategory}_{i+1}.csv\"\n",
    "            full_path_file_name = f\"s3://{s3_bucket}/{s3_key}/{file_name}\"\n",
    "\n",
    "            # Write the CSV file\n",
    "            # Optional, without WR: subset.iloc[start_row:end_row].to_csv(full_path_file_name, index=False)\n",
    "            df_output = subset.iloc[start_row:end_row]\n",
    "            wr.s3.to_csv(df_output, full_path_file_name, index=False)\n",
    "\n",
    "            # Write Metadata Filter files\n",
    "            file_metadata = {\n",
    "                \"metadataAttributes\": {\n",
    "                    \"category\" : category,\n",
    "                    \"subcategory_1\" : sub_category,\n",
    "                    \"file_part\" : i+1,\n",
    "                    \"total_files\" : num_files\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Metadata File\n",
    "            s3_metadata_file = f\"{s3_key}/{file_name}.metadata.json\"\n",
    "\n",
    "            # Write JSON metadata to S3. Do not return \"response\", as it's too much for logging/printing\n",
    "            response = s3_client.put_object(Bucket=s3_bucket, Key=s3_metadata_file, Body=json.dumps(file_metadata))\n",
    "\n",
    "        return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error while writing data: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define params. Reduce threads according to the environment and dataset size\n",
    "num_rows_per_file = 10\n",
    "local_number_of_threads = 30\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=local_number_of_threads) as executor:\n",
    "    tasks = [(category, sub_category, s3_bucket, s3_key, num_rows_per_file) for category, sub_category in df[['category', 'subcategory_1']].drop_duplicates().itertuples(index=False)]\n",
    "    executor.map(write_csv_to_s3, tasks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
